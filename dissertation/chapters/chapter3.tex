% !TeX root = ..\dissertation.tex

\chapter{Opening the black box}

\begin{todo}
	explain the idea \\
	ANNs take a long time to train (calculus is hard) but can make predictions very quickly (arithmetic is easy) \\
	this means that once trained, we can get lots of data to give to the regression algorithm or GP or whatever  
\end{todo}

\section{State of the art}

The fact that \note{predicting from} an \ac{ANN} consist of many simple operations in a hierarchical structure suggests that it could be possible to mathematically or statistically explain or approximate their output.

\textcite[13]{cortez2013} trained a deep neural network on images and attempted to \qt{invert} it to reconstruct the original inputs.
They found that each successive layer abstracts the data, but isolating the specific neurons responsible for specific objects or ideas was more difficult.
Sensitivity analysis has also been used to evaluate the importance of each input to an \ac{ANN} in the final output, and the results visualised as a decision tree where each branch represents a range of input values~

There appears to be no literature on using \acp{GP} as a way to better understand how deep learning algorithms work or as a way to understand their structure, although there has been a fierce debate over the advantages of both \acp{ANN} and \acp{GP} since at least the 1990s.
\textcite[65--66]{rasmussen1997} gave evidence that \acp{GP} are a valid replacement for \acp{ANN} in nonlinear problems with fewer than 1000 datapoints, although due to advances in processing power, \ac{ANN} algorithms and \ac{GP} techniques, this recommendation will likely have changed.
\textcite[25]{mackay1997} gave an example of \acp{GP} being used for binary classification, in a similar way to \acp{ANN}.
When \ac{GPR} and \acp{ANN} are compared, \reword{the main advantages mentioned} are usually faster convergence and confidence or credible intervals for the predictions, which \acp{ANN} cannot do~\autocite{herbrich2003}.

\section{Multiple regression}

One method of opening the black box of \ac{ML} is to use multiple regression,where many basis functions are calculated and traditional regression is used on the resulting matrix in an attempt to uncover which are the \qt{true} functions.
The number of basis functions can be increased arbitrarily, but will increase the fitting time. 

In this case, the \reword{basis functions/basis vectors[?]} \(x\), \(x^2\), \(\sin(x)\), \(\sin(2x)\), \(\sin(x/2)\), \(\cos(x)\), \(\cos(2x)\) and \(\cos(x/2)\) were used.
% The desired output would be that \(x + 5 \sin(x)\)
This produced a complex model with many unwanted coefficients close to zero, but also many others far from zero, as seen in Table~\ref{tab:multiple-regression}.

\begin{todo}
	Expand on why I chose these basis functions/vectors.
\end{todo}

\includetable{multiple-regression}

\begin{todo}
	This is slightly cheating, but is feasible and a common technique. Find a source for this.
\end{todo}

\subsection{Stepwise regression}

It is then possible to use stepwise regression to reduce the number of parameters in this linear model by removing them and comparing.
In this case, the variables were selected using the \ac{AIC}, which penalises the fit for having a higher number of coefficients.
The results are seen in Table~\ref{tab:stepwise-regression}.

\includetable{stepwise-regression}

While the relevant estimators \(x\) and \(\sin(x)\) were identified and their coefficients fairly accurately estimated, a few other variables were also identified as significant.
This method is only likely to work with a perfect or near perfect fit with no noise, which is unrealistic for real applications.

\begin{todo}
	expand this part
\end{todo}

\subsection{\acl{LASSO}}

Alternatively, we can use the \ac{LASSO} method to select the significant variables from the full model.
The \ac{LASSO} method constrains the sum of the absolute values of the model parameters, regularising the least influential parameters to zero.
We vary the hyperparameter \(\lambda\) to change \reword{how regularised the coefficients are}, as seen in Figure~\ref{fig:lasso-lambda}.
We then use leave-one-out \ac{CV} to find the optimal hyperparameter \(\lambda\), the results of which can be seen in Table~\ref{tab:lasso-coefs}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{lasso-lambda}
	\caption{Changing the hyperparameter \(\lambda\).}
	\label{fig:lasso-lambda}
\end{figure}

\includetable{lasso-coefs}

\section{Gaussian processes}

One proposed way to better understand \ac{DL} is by using \acp{GP} to approximate the output of a \ac{DL} algorithm.
It has been shown that \reword{as the number of neurons in a layer of an} \ac{ANN} approaches infinity, the fit of the layer approaches that of a \ac{GP}.

\begin{todo}
I need to find a source for that
\end{todo}

\reword{In a similar way to how a} univariate normal distribution with a mean and variance can be generalised to a multivariate normal distribution with a mean vector and a normal vector, a \ac{GP} is the limit of extending a multivariate normal distribution to infinite dimensions, with a mean function and covariance function (sometimes also called a kernel in \ac{ML} environments) which depends on the distance between two points.

A random vector \(X \in \reals{}^n\) is distributed with a multivariate normal distribution in \(n\) dimensions with mean vector \(\vec{\mu} \in \reals{}^n\) and covariance matrix \(K \in \reals^{n \times n}\) if \(X \distributed \normdist(\vec{\mu},\, K)\).
In a similar way, \(Y\) is a \ac{GP} with a mean function \(\mu\) and a covariance function \(k\) where \(k(x_1, x_2) = k(\left|x_1 - x_2\right|)\) if \(Y \distributed \gp(\mu,\, k)\).
Because a \ac{GP} is an extension of a multivariate normal distribution, any finite subset of points from a \ac{GP} has a multivariate normal distribution~\autocite[515]{williams1996}.

\begin{todo}
	Explain how a GP will help us understand the ANN. \\
	GPs have statistical properties that we understand, so if we can fit a GP and then calculate these properties we can apply our knowledge to the ANN.
\end{todo}

\subsection{Using Gaussian process regression}

Because \acp{GP} are very flexible, applying a \ac{GP} to the output of the \ac{ANN} is likely to result in a close fit.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{gp-fit}
	\caption{The fit of the \ac{GP}.}
	\label{fig:gp-fit}
\end{figure}

The fit of the \ac{GP} is almost perfect, as expected.

\begin{todo}
	give some sort of quantitative explanation of how well the GP has fitted \\
	explain how the GP can now be used
\end{todo}

