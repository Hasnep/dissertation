% !TeX root = ..\dissertation.tex

\chapter{Opening the black box}

\reword{Neural networks take a long time to train (calculus is hard) but can make predictions very quickly (arithmetic is easy)}.
This means that once trained, we can very quickly get lots of data to \reword{give to the regression model or GP or whatever}.
This means we're not limited by a lack of data from the neural network.

\section{State of the art}

The fact that \reword{predicting from} a neural network consists of many simple operations in a hierarchical structure suggests that it could be possible to mathematically or statistically explain or approximate their output.

\textcite{mahendran2014} trained a deep neural network on images and attempted to \qt{invert} it to reconstruct the original inputs.
They found that each successive layer abstracts the data, but isolating the specific neurons responsible for specific objects or ideas was more difficult.

Sensitivity analysis has also been used to evaluate the importance of each input to a neural network in the final output, and the results visualised as a decision tree where each branch represents a range of input values~\autocite[13]{cortez2013}.

There appears to be no literature on using \acp{GP} as a way to better understand how deep learning algorithms work or as a way to understand their structure, although there has been a fierce debate over the advantages of both neural networks and \acp{GP} since at least the 1990s.
\textcite[65--66]{rasmussen1997} gave evidence that \acp{GP} are a valid replacement for neural networks in nonlinear problems with fewer than 1000 datapoints, although due to advances in processing power, neural network algorithms and \ac{GP} techniques, this recommendation will likely have changed.
\textcite[25]{mackay1997} gave an example of \acp{GP} being used for binary classification, in a similar way to neural networks.
When \ac{GPR} and neural networks are compared, \reword{the main advantages mentioned} are usually faster convergence and confidence or credible intervals for the predictions, which neural networks cannot do~\autocite{herbrich2003}.

\section{Multiple regression}

One method of opening the black box of machine learning is to use multiple regression on a series of basis functions.
Many basis functions are calculated and traditional linear regression is used on the resulting matrix of predictors in an attempt to uncover which are the \qt{true} functions.
This method requires that \reword{enough basis functions are used that the correct basis functions are in the model}.
The number of basis functions can be increased arbitrarily, but this will increase the fitting time.
This technique also requires regularisation to reduce the number of terms in the model, as there will likely be many terms with small coefficients that are determined to be significant.

\begin{todo}
	Danny seemed unconvinced by this. Find a source to legitimise this. \\
	Why does this not violate the assumption that our predictors are independent?
\end{todo}

In this case, the basis functions \(x\), \(x^2\), \(\sin(x)\), \(\sin(2x)\), \(\sin(x/2)\), \(\cos(x)\), \(\cos(2x)\) and \(\cos(x/2)\) were used, where the desired output is that \(x\) will have a coefficient of 1 and \(\sin(x)\) will have a coefficient of 5.
In this situation, the exact \qt{correct} terms are known, but in a \reword{real life apllication}, many more terms with different frequencies and offsets of \(\sin\) and \(\cos\) could be fitted and then removed using regularisation.
Due to the imperfect fit of the neural network, it is unlikely that the desired coefficients will be selected exactly, so other coefficients will \reword{try to fix any phase problems}.

\begin{todo}
	Expand on why I chose these basis functions.
\end{todo}

Table~\ref{tab:multiple-regression} shows a summary of the fit of the regression model, where there are many unwanted coefficients close to zero, but also many others far from zero.
This model requires regularisation to reduce the number of terms and to remove the terms with coefficients very close to zero.

\subsection{Stepwise regression}

One way to regularising the model is to use stepwise regression to reduce the number of parameters by adding or removing one at a time and comparing the fit of the hierarchical models.
For this scenario, it makes sense to use forward selection, where the initial model is empty and a term is added at each step, rather than backward selection, which starts with a full model and removes a term at each step.
\reword{This is because we're trying to pick out the imprtant terms from a big lit of possibilities rather than remove a few unimportant terms.} 
In this case, the variables were selected using the \ac{AIC}, which penalises the fit for having a higher number of coefficients.
The results of this regularisation are seen in Table~\ref{tab:stepwise-regression}.

The coefficients are slightly different, as seen in Table~\ref{tab:compare-coefs}.

\includetable{stepwise-regression}

While the relevant estimators \(x\) and \(\sin(x)\) were identified and their coefficients fairly accurately estimated, a few other variables were also identified as significant.
This method is only likely to work with a perfect or near perfect fit with no noise, which is unrealistic for real applications.

\begin{todo}
	expand this part
\end{todo}

\subsection{LASSO}

Alternatively, we can use \ac{LASSO} regularisation to select the significant variables from the full model.
The \ac{LASSO} constrains the sum of the absolute values of the model parameters, regularising the least influential parameters to zero.
We vary the hyperparameter \(\lambda\) to change the trade off between simplicity and fit accuracy, as seen in Figure~\ref{fig:lasso-lambda}.
We then use leave-one-out cross-validation to find the optimal hyperparameter \(\lambda\), the results of which can be seen in Table~\ref{tab:lasso-coefs}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{lasso-lambda}
	\caption{The effect on the coefficients when changing the regularisation hyperparameter \(\lambda\).}
	\label{fig:lasso-lambda}
\end{figure}

\includetable{lasso-coefs}

\subsection{Comparison}

Table~\ref{tab:compare-coefs} shows a comparison between the coefficients selected by the full model and the two regularisation methods.

\includetable{compare-coefs}

\section{Gaussian processes}

One proposed way to better understand deep learning is by using \acp{GP} to approximate the output of a deep learning algorithm.
It has been shown that \reword{as the number of neurons in a layer of a} neural network approaches infinity, the fit of the layer approaches a \ac{GP}~\autocite{neal1996}.

\reword{In a similar way to how a} univariate normal distribution with a mean and variance can be generalised to a multivariate normal distribution with a mean vector and a normal vector, a \ac{GP} is the limit of extending a multivariate normal distribution to infinite dimensions, with a mean function and covariance function (sometimes also called a kernel in machine learning environments) which depends on the distance between two points.

A random vector \(X \in \reals{}^n\) is distributed with a multivariate normal distribution in \(n\) dimensions with mean vector \(\vec{\mu} \in \reals{}^n\) and covariance matrix \(K \in \reals^{n \times n}\) if \(X \distributed \normdist(\vec{\mu},\, K)\).
In a similar way, \(Y\) is a \ac{GP} with a mean function \(\mu\) and a covariance function \(k\) where \(k(x_1, x_2) = k(\left|x_1 - x_2\right|)\) if \(Y \distributed \gp(\mu,\, k)\).
Because a \ac{GP} is an extension of a multivariate normal distribution, any finite subset of points from a \ac{GP} has a multivariate normal distribution~\autocite[515]{williams1996}.

\begin{todo}
	Explain how a GP will help us understand the neural network. \\
	GPs have statistical properties that we understand, so if we can fit a GP and then calculate these properties we can apply our knowledge to the neural network.
\end{todo}

\subsection{Using Gaussian process regression}

Because \acp{GP} are very flexible, applying a \ac{GP} to the output of the neural network is likely to result in a close fit.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{gp-fit}
	\caption{The fit of the \ac{GP} (\gpcolour) and the true function (\truthcolour).}
	\label{fig:gp-fit}
\end{figure}

The fit of the \ac{GP} is almost perfect, as expected.

\begin{todo}
	give some sort of quantitative explanation of how well the GP has fitted \\
	explain how the GP can now be used
\end{todo}

