% !TeX root = ..\dissertation.tex

\chapter{Opening the Black Box}

\section{State of the art}

The fact that an \ac{ANN} consist of many simple operations in a hierarchical structure suggests that it could be possible to mathematically or statistically explain or approximate their output.

\textcite[13]{cortez2013} trained a deep neural network on images and attempted to \qt{invert} it to reconstruct the original inputs.
They found that each successive layer abstracts the data, but isolating the specific neurons responsible for specific objects or ideas was more difficult.
Sensitivity analysis has also been used to evaluate the importance of each input to an \ac{ANN} in the final output, and the results visualised as a decision tree where each branch represents a range of input values~

There appears to be no literature on using \acp{GP} as a way to better understand how deep learning algorithms work or as a way to understand their structure, although there has been a fierce debate over the advantages of both \acp{ANN} and \acp{GP} since at least the 1990s.
\textcite[65--66]{rasmussen1997} gave evidence that \acp{GP} are a valid replacement for \acp{ANN} in nonlinear problems with fewer than 1000 datapoints, although due to advances in processing power, \ac{ANN} algorithms and \ac{GP} techniques, this recommendation will likely have changed.
\textcite[25]{mackay1997} gave an example of \acp{GP} being used for binary classification, in a similar way to \acp{ANN}.
When \ac{GPR} and \acp{ANN} are compared, the main advantages mentioned are usually faster convergence and confidence or credible intervals for the predictions, which \acp{ANN} cannot do~\autocite{herbrich2003}.
\note{Rewrite that sentence}

\section{Multiple regression}

One method of opening the black box of machine learning is to use multiple regression.

In this case, \(x\), \(x^2\), \(\sin(x)\), \(\sin(2x)\), \(\sin(x/2)\), \(\cos(x)\), \(\cos(2x)\) and \(\cos(x/2)\) were used.
This produced a complex model with many coefficients close to zero.

\begin{todo}
	Expand on why I chose these basis vectors.
\end{todo}

\includetable{multiple-regression}

\begin{todo}
	This is slightly cheating, but is feasible and a common technique. Find a source for this.
\end{todo}

\subsection{Stepwise regression}

It is then possible to use stepwise regression to reduce the number of parameters in this linear model.
In this case, the variables were selected using the \ac{AIC}.

\includetable{stepwise-regression}

While the relevant estimators \(x\) and \(\sin(x)\) were identified and their coefficients fairly accurately estimated, a few other variables were also identified as significant.
This method is only likely to work with a perfect or near perfect fit with no noise, which is unrealistic for real applications.

\subsection{LASSO}

Alternatively, we can use the \ac{LASSO} method to select the significant variables from the full model.
The \ac{LASSO} method constrains the sum of the absolute values of the model parameters, regularising the least influential parameters to zero.
We vary the hyperparameter \(\lambda\) to change how regularised the coefficients are, as seen in Figure~\ref{fig:lasso-lambda}.
We then use \(k\)-fold cross validation to find the optimal hyperparameter \(\lambda\).

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{lasso-lambda}
	\caption{Changing the hyperparameter \(\lambda\).}
	\label{fig:lasso-lambda}
\end{figure}

\includetable{lasso-coefs}

\section{Gaussian processes}

We can use \acp{GP} to try and model the output of the \ac{ANN}.
\acp{GP} are an extension of the multivariate normal distribution to an infinite dimensional process with a mean function and covariance function instead of a mean vector and covariance matrix.

\subsection{Using Gaussian process regression}

Because \acp{GP} are very flexible, applying a GP to the output of the ANN is likely to result in a close fit.

\begin{todo}
	The limit of an ANN is a GP.
\end{todo}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\figwidth]{example-image-golden.pdf}
	\caption{The fit of the \ac{GP}.}
	\label{fig:gp-fit}
\end{figure}
