% !TeX root = ..\dissertation.tex

\chapter{Opening the Black Box}

\section{Previous papers on this subject}

\section{Regression}

One method of opening the black box of machine learning is to use multiple regression.

\subsection{Linear regression}

In this case, \(x\), \(x^2\), \(\sin(x)\), \(\sin(2x)\), \(\sin(x/2)\), \(\cos(x)\), \(\cos(2x)\) and \(\cos(x/2)\) were used.
This produced a complex model with many coefficients close to zero.

\begin{todo}
	Expand on why I chose these basis vectors.
\end{todo}

\includetable{multiple-regression}

\begin{todo}
	This is slightly cheating, but is feasible and a common technique. Find a source for this.
\end{todo}

\subsection{Stepwise regression}

It is then possible to use stepwise regression to reduce the number of parameters in this linear model.
In this case, the variables were selected using the \ac{AIC}.

\includetable{stepwise-regression}

While the relevant estimators \(x\) and \(\sin(x)\) were identified and their coefficients fairly accurately estimated, a few other variables were also identified as significant.
This method is only likely to work with a perfect or near perfect fit with no noise, which is unrealistic for real applications.

\subsection{LASSO}

Alternatively, we can use the \ac{LASSO} method to select the significant variables from the full model.
The \ac{LASSO} method constrains the sum of the absolute values of the model parameters, regularising the least influential parameters to zero.
We vary the hyperparameter \(\lambda\) to change how regularised the coefficients are, as seen in Figure~\ref{fig:lasso-lambda}.
We then use \(k\)-fold cross validation to find the optimal hyperparameter \(\lambda\).

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\figwidth]{example-image-golden.pdf}
	\caption{Changing the hyperparameter \(\lambda\).}
	\label{fig:lasso-lambda}
\end{figure}

\begin{todo}
	table of LASSO coefficients
\end{todo}

\section{Gaussian processes}

We can use \acp{GP} to try and model the output of the \ac{ANN}.
\acp{GP} are an extension of the multivariate normal distribution to an infinite dimensional process with a mean function and covariance function instead of a mean vector and covariance matrix.

\subsection{Using Gaussian process regression}

Because \acp{GP} are very flexible, applying a GP to the output of the ANN is likely to result in a close fit.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\figwidth]{example-image-golden.pdf}
	\caption{The fit of the \ac{GP}.}
	\label{fig:gp-fit}
\end{figure}
