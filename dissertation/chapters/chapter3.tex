% !TeX root = ../dissertation.tex

\chapter{Opening the black box}

The fact that predicting from a neural network consists of many simple operations in a hierarchical structure suggests that it could be possible to mathematically or statistically explain or approximate their output.
Directly understanding the meaning of each weight in a neural network is impossible for a human.
Even the neural network used to learn the simple example in this project seen in Figure~\ref{fig:final-ann-architecture} is far too complex to keep track of.

\section{Current and past research}

When neural networks were first trained on image classification problems, it was thought that each layer would successively combine features from the previous layer to form more complex structures.
For example, a common machine learning problem is designing a neural network for classifying handwritten digits where the input is a vector of length \(n^2\) representing the brightness of each pixel in an \(n \times n\) image.
A designer might plan for the first layer to pick out clusters of pixels, then the next layer would combine adjacent clusters into lines, the next layer would combine lines into corners, and the final layer would identify combinations of lines and corners as digits.
However, once a neural network is trained on such a problem, each neuron is activated by a seemingly random set of pixels spread across the image.
For this reason, it is almost impossible to give meaning to any neuron in the neural network as was originally hypothesised.

\textcite{mahendran2014} trained a deep neural network on images and attempted to \qt{invert} it to reconstruct the original inputs.
They found that each successive layer abstracts the data, but isolating the specific neurons responsible for specific objects or ideas was more difficult.

Sensitivity analysis has also been used to evaluate the importance of each input to a neural network in the final output, and the results visualised as a decision tree where each branch represents a range of input values~\autocite[13]{cortez2013}.

\section{Multiple regression} \label{sec:multiple-regression}

An ideal way to understand a deep neural network would be to find the form of a function that describes the output of the neural network.
One way of doing this is to use multiple regression with a series of basis functions on the output of the neural network.
A new dataset is generated by using the neural network to predict the target values in the domain that it was trained on.
Then, many basis functions are calculated and traditional linear regression is used on the resulting matrix of predictors in an attempt to uncover which are the \qt{true} functions that the neural network is trying to approximate.
The number of basis functions can be increased arbitrarily, but this will increase the fitting time.
This technique also requires regularisation to reduce the number of terms in the model, as there will likely be many terms with small coefficients that are determined to be significant.

In this case, the basis functions \(1\), \(x\), \(x^2\), \(\sin(x)\), \(\sin(2x)\), \(\sin(x/2)\), \(\cos(x)\), \(\cos(2x)\) and \(\cos(x/2)\) were used, where the desired output is that \(x\) and \(\sin(x)\) will be selected as significant and ideally have coefficients close to 1 and 5 respectively and the other terms will have coefficients close to zero.
In this situation, the exact \qt{true} terms are known, but in a more realistic application where the form of the input function is completely unknown, many more terms could be fitted including higher order polynomials and interactions between predictors.
Due to the imperfect fit of the neural network, it is unlikely that the desired coefficients will be selected exactly, so other terms will have non-zero coefficients to slightly adjust the model's fit.

Table~\ref{tab:multiple-regression} shows a summary of the fit of the regression model.
There are many terms with coefficients close to zero which could be ignored as they do not contribute much to the fit, although almost all of the terms have \(t\)-values in the extreme tails of the distribution.

\includetable{multiple-regression}

This model requires regularisation to reduce the number of terms and to determine which of the insignificant terms to remove.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{multiple-regression-fit}
	\caption{The fit of the multiple regression model.}
	\label{fig:multiple-regression-fit}
\end{figure}

\subsection{Stepwise regression}

One way to regularise the model is to use stepwise regression to reduce the number of parameters.
\reword{It does this by} adding or removing one term at a time and comparing the fit of the nested models.
In this case, the variables were selected using the \ac{AIC}, which penalises the fit for having a higher number of coefficients.
For this example, forward selection, where the initial model is empty and a term is added at each step, and backward selection, which starts with a full model and removes a term at each step, were used, both arrived at similar models, but the method with the lowest \ac{AIC} was to use both forward and backward selection.
Table~\ref{tab:stepwise-regression} shows a summary of the reduced model.
\includetable{stepwise-regression}
The intercept and \(\cos(2x)\) terms were removed from the full model.
The coefficients of some of the reduced model are slightly different from the full model to compensate for the removed terms, as seen in Table~\ref{tab:compare-coefs}.

While the relevant estimators \(x\) and \(\sin(x)\) were identified and their coefficients fairly accurately estimated, a few other variables were also identified as significant.
This method is only likely to work with a perfect or near perfect fit with no noise, which is unrealistic for real applications.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{stepwise-regression-fit}
	\caption{The fit of the stepwise model.}
	\label{fig:stepwise-regression-fit}
\end{figure}

\subsection{LASSO}

A more modern alternative to stepwise regression is \ac{LASSO} regularisation.
The \ac{LASSO} constrains the sum of the absolute values of the model parameters, regularising the least influential parameters to zero.
The hyperparameter \(\lambda\) controls the trade off between model simplicity and fit accuracy, as seen in Figure~\ref{fig:lasso-lambda}.
Then leave-one-out cross-validation is used to find the optimal value for \(\lambda\), the results of which can be seen in Table~\ref{tab:lasso-coefs}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{lasso-lambda}
	\caption{The effect on the coefficients when changing the regularisation hyperparameter \(\lambda\) and the optimised value of \(\lambda\) (dashed red).}
	\label{fig:lasso-lambda}
\end{figure}

\includetable{lasso-coefs}

The \ac{LASSO} has removed many more terms than the stepwise regression, as seen in Table~\ref{tab:compare-coefs}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{lasso-fit}
	\caption{The fit of the \ac{LASSO} (\lassocolour) and the true function (\truthcolour).}
	\label{fig:lasso-fit}
\end{figure}

\subsection{Comparison of linear regression models}

Table~\ref{tab:compare-coefs} shows a comparison between the coefficients selected by the full model and the two regularised models.
The \ac{LASSO} model removed many more terms that had coefficients close to zero than the stepwise regression model, and seems to have shown that the form of the equation for the neural network consists mainly of \(x\) and \(\sin(x)\) terms, with a small negative intercept.
The \(\sin(x/2)\) coefficient is so small that it makes almost no difference to the model.
Interpreting the stepwise regression model is slightly more difficult as there are more terms.
The \(x\) and \(\sin(x)\) terms make up the majority of the fit, and most other coefficients are close to zero, leaving only a few phase shifting terms such as \(\sin(x/2)\).

\includetable{compare-coefs}

\section{Gaussian processes}

One proposed way to better understand deep learning is by using \acp{GP} to approximate the output of a deep learning algorithm.
One reason for this is because it has been shown that the fit of a neuron tends to a \ac{GP} as the number of inputs approaches infinity~\autocite{neal1996}.

Similar to how a univariate normal distribution with a mean and variance can be generalised to a multivariate normal distribution with a mean vector and a normal vector, a \ac{GP} is the limit of extending a multivariate normal distribution to infinite dimensions, with a mean function and covariance function (sometimes also called a kernel in machine learning environments) which depends on the distance between two points.

A random vector \(X \in \reals{}^n\) is distributed with a multivariate normal distribution in \(n\) dimensions with mean vector \(\vec{\mu} \in \reals{}^n\) and covariance matrix \(K \in \reals^{n \times n}\) if \(X \distributed \normdist(\vec{\mu},\, K)\).
In a similar way, \(Y\) is a \ac{GP} with a mean function \(\mu\) and a covariance function \(k\) where \(k(x_1, x_2) = k(\left|x_1 - x_2\right|)\) if \(Y \distributed \gp(\mu,\, k)\).
Because a \ac{GP} is an extension of a multivariate normal distribution, any finite subset of points from a \ac{GP} has a multivariate normal distribution~\autocite[515]{williams1996}.

There appears to be no literature on using \acp{GP} as a way to better understand how deep learning algorithms work or as a way to understand their structure, although there has been a fierce debate over the advantages of both neural networks and \acp{GP} since at least the 1990s.
\textcite[65--66]{rasmussen1997} gave evidence that \acp{GP} are a valid replacement for neural networks in nonlinear problems with fewer than 1000 datapoints, although due to advances in processing power, neural network algorithms and \ac{GP} techniques, this recommendation will likely have changed.
\textcite[25]{mackay1997} gave an example of \acp{GP} being used for binary classification, in a similar way to neural networks.
The most commonly cited advantages of \acp{GP} are faster convergence and the ability to calculate confidence intervals (or credible intervals) for predictions, which neural networks cannot do~\autocite{herbrich2003}.

\subsection{Using Gaussian process regression}

A \ac{GP} can be fitted to the output of a neural network in the same way as the multiple regression model in Section~\ref{sec:multiple-regression}.
\acp{GP} have well understood statistical properties such as a length-scale and robust sensitivity analysis techniques can be applied to the fit of the neural network to provide another human understandable perspective to the neural network.
In this example, a \ac{GP} was fitted to a smaller set of 256 datapoints produced by the neural network, as the improvement to fit when using the full 1024 points did not warrant the significantly longer fitting time\footnote{Software: R 3.5.3, \qt{mlegp} package~\autocite{dancik2008}; Hardware: Intel Core i5-4210U processor, 8GB RAM}.
The \ac{GP} was fitted with no nugget, so that it would interpolate between the points rather than smooth them, although there are likely contexts where some smoothing would be useful.
Because \acp{GP} are very flexible, applying a \ac{GP} to the output of the neural network is likely to result in a close fit.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{gp-fit}
	\caption{The fit of the \ac{GP} and 99\% confidence interval (\gpcolour) and the neural network (\nncolour). The confidence interval is so narrow that it is hidden and the fit of the \ac{GP} is so close to the neural network's fit that it can be difficult to see.}
	\label{fig:gp-fit}
\end{figure}

The fit of the \ac{GP} to the neural network is almost perfect.

\section{Combining regression and Gaussian processes}

The fit of the \ac{LASSO} model is not perfect, as seen in Figure~\ref{fig:lasso-fit}.
One possible way to improve the fit of this model is to use a \ac{GP} to capture the residuals of the \ac{LASSO} model.
The residuals of the model are seen in Figure~\ref{fig:gp-resids-fit}, where a \ac{GP} has been fitted.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{gp-resids-fit}
	\caption{The fit of the \ac{GP} to the residuals of the \ac{LASSO} model.}
	\label{fig:gp-resids-fit}
\end{figure}

Figure~\ref{fig:combined-fit} shows the two models summed together.
Judging by eye, the fit is essentially perfect, and using this method gives a good understanding of the form of the formula for the neural network's output and also some information about the residuals captured by the \ac{GP}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{combined-fit}
	\caption{The fit of the combined \ac{LASSO} and \ac{GP} model (\combinedcolour) and the neural network (\nncolour).}
	\label{fig:combined-fit}
\end{figure}
