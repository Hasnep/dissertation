% !TeX root = ..\dissertation.tex

\chapter{Machine Learning}

We are in an age where there is so much data that in many cases, hand building a model to analyse, find patterns in and draw conclusions from the data is unfeasible, and so we use computers to analyse it in a process called \qt{Machine Learning} (ML)~\autocite[1]{murphy2012}.
Computers are able to do these tasks by combining information in nonlinear ways, but become more powerful when the output of one nonlinear process is fed into another nonlinear process, abstracting the raw data to a higher level.
\qtc[436]{lecun2015}{With the composition of enough such transformations, very complex functions can be learned}.

\section{Neural Networks}

\subsection{The Neuron}

\begin{todo}
	Each neuron performs a weighted sum 
\end{todo}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{A diagram of an example artificial neuron.}
	\label{fig:neuron-example}
\end{figure}

\subsection{Neural network layers and Backpropagation}

\begin{todo}
	Neurons are connected in layers, Backpropagation is used to optimise the weights
\end{todo}

\section{Deep Learning}

\begin{todo}
	DL is just ML with more layers 
\end{todo}

\note{Rewrite this to more slowly introduce deep learning and be more generally about machine learning than ANNs}
As computing power has increased exponentially following Moore's law, machine learning has become a more viable and effective method of prediction.
In the 21st century, success has been found in increasing the number of layers in a neural network or other machine learning method rather than the complexity of each layer.
These \qt{deep neural networks} \qtc[438]{lecun2015}{can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details}.

\section{Modern techniques/tools}

\ac{DL} has even made its way into consumer products.
Many modern phones have facial recognition software built in, and \ac{AI} voice assistants are becoming more common in homes.

\ac{DL} is also becoming more accessible, with the release of Google's \qt{Tensorflow} package for \ac{DL}~\autocite{abadi2016}, which is now available as \qt{Keras}, a user friendly package for Python~\autocite{chollet2015} and R~\autocite{allaire2018}.

\section{Example}

To demonstrate the possibility of using statistical methods to understand the process of deep learning, we use a simple function \(f(x) = x + 5 \sin(x)\).
256 evenly spread datapoints were taken from this function, and noise following \(\epsilon \distributed \normdist(0, 0.1)\) was applied.

\begin{todo}
Possibly also use a more complex example?
\end{todo}

\subsection{Training a neural network}

This function was learnt by a neural network with 8 layers, each with 10 neurons and the \(\tanh(\cdot)\) activation function, except for the final layer which used a linear activation function.
The result of this learning is seen in Figure~\ref{fig:ann-output}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{The output of the neural network.}
	\label{fig:ann-output}
\end{figure}

\begin{todo}
Information on choosing the right size NN
\end{todo}

\subsection{Ordering of the datapoints}

Due to the form of stochastic gradient descent used to train the model, if the order of the datapoints is not randomised, then the optimisation algorithm is significantly more likely to get stuck in a local minimum.
An example of this is seen in Figure~\ref{fig:ann-order}, where the same neural network has been trained on the example data which has been shuffled, reversed, randomised and separated.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{The output of the same neural network trained on the same data but ordered differently.}
	\label{fig:ann-order}
\end{figure}

An example of this being used can be seen in \citeauthor{bengio2009}.
