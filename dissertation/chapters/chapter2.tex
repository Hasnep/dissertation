% !TeX root = ..\dissertation.tex

\chapter{Machine Learning}

The rise in success that \ac{ML} and \ac{DL} \reword{have seen is down to two factors: the increase in computing power and the increase in data being collected rising.}

\section{Artificial Neural Networks}

One of the most popular types of \ac{ML} algorithm is the \ac{ANN}, a very general algorithm that has \reword{seen great sucess} in performing regression and classification tasks, as well as in other contexts such as reinforcement learning.
It was chosen for this \reword{experiment} \note{?} because \note{finish this sentence}

\subsection{The Perceptron}

In 1957, psychologist Frank Rosenblatt proposed a stochastic electronic brain model, which he called a \qt{perceptron}~\autocite{rosenblatt1957}.
At the time, most models of the brain were deterministic algorithms which could recreate a single neural phenomenon such as memorisation or object recognition.
Rosenblatt's \reword{biggest criticism} of these models was that while a deterministic algorithm could perform a single task perfectly, unlike a biological brain, it could not be generalised to perform any more tasks without substantial changes.
He described deterministic models of the brain as \qtc[387]{rosenblatt1958}{amount[ing] simply to logical contrivances for performing particular algorithms \cut{} in response to sequences of stimuli}.
Another way he wanted his synthetic brain to mirror biological brains was the property of redundancy, the ability to have pieces removed and still function, which is not possible for deterministic algorithms where even a small change to a circuit or a line of code can stop all functionality.

It was a commonly held belief that deterministic algorithms \qtc[387]{rosenblatt1958}{would require only a refinement or modification of existing principles}, but Rosenblatt questioned this idea, believing that the problem needed a more fundamental re-evaluation.
At the heart of his idea was the \qt{perceptron}, which could -- through repeated training and testing -- receive a set of inputs and reliably give a correct binary response.
The perceptron was later generalised to the concept of the (artificial) neuron (also called a \qt{unit}), which, instead of only giving a binary output, maps a number of real inputs to a single real output value.

\subsection{Artificial Neurons}

A neuron is defined by its weight vector~\(\vec{w}\), its bias~\(b\) and its activation function (or \qt{limiter function})~\(\phi(\x)\).

The stages of a neuron, seen in Figure~\ref{fig:neuron-example}, are:
\begin{enumerate}
	\item For an input vector \(\vec{x} \in \reals^n\) and a weight vector \(\vec{w} \in \reals^n\), take a weighted sum of the inputs, called a \qt{linear combiner}.
	      \[ \text{linear combiner} = \sum_{i=1}^{n}{x_i w_i} \]
	\item Then, the bias \(b \in \reals{}\) is added, which translates the output to a suitable range.
	      The bias controls how large the weighted sum needs to be to \qt{activate} the neuron, so this is called the pre-activation \((v)\).
	      \[ \text{pre-activation} = v = \sum_{i=1}^{n}{x_i w_i} + b \]
	\item Finally, the activation function \(\phi(\x)\) is applied, which restricts the output to a range and introduces nonlinearity to the system.
	      The activation function must be differentiable if the gradient descent method is used (see Section~\ref{sec:backpropagation}).
	      \[ \text{neuron output} = \hat{y} = \phi\left(\sum_{i=1}^{n}{x_i w_i} + b \right) \]
\end{enumerate}

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{neuron-example}
	\caption{A diagram of an example artificial neuron.}
	\label{fig:neuron-example}
\end{figure}

% \subsubsection{Activation functions}

Early examples of artificial neurons were built in hardware \reword{with the output being a light that was either on or off.}
This is equivalent to the \(\operatorname{sign}(\x)\) function, which is now only used for binary classification problems.
It is not useful for connecting to another neuron, as information about the magnitude of the output is lost.
Commonly, the sigmoid function \(S(x) = 1/(1 + e^{-x})\), the \(\tanh(\x)\) function or the \qt{softmax} function (a generalisation of the logistic function to higher dimensions) are used.
More recently, a popular used activation function for \ac{DL} (see Section~\ref{sec:deep-learning}) is the \ac{ReLU} function~\autocite{ramachandran2017}, which is defined as the positive part of its input \(\operatorname{ReLU}(x) = \max(0, x)\).
The \ac{ReLU} function was designed to be analogous to how a biological neuron can be either inactive or active, although why it \reword{works as well as it does} is not well understood.
If the activation function is the identity function, then optimising a neuron is equivalent to performing linear regression.

\subsection{Training} \label{sec:backpropagation}

In the case of a \qt{feedforward} \ac{ANN}, the neurons are connected in sequential groups called layers, where each layer only receives information from the previous layer and only passes information to the subsequent layer.
Convolutional and recurrent neural networks also exist in which the layers are not connected in series.
A fully connected \ac{ANN} is one where every neuron in a layer is connected to every layer on the subsequent layer.
% As seen in Figure~\ref{fig:neural-network-example}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{neural-network-example}
	\caption{An example of the structure of a fully connected feedforward neural network.}
	\label{fig:neural-network-example}
\end{figure}

The weights in a neural network are optimised (or \qt{trained}) using the backpropagation algorithm.
The weights \(\vec{w} = w_1, \dots, w_N\) are randomly initialised.
One step (called an \qt{epoch}) in the backpropagation algorithm is defined as:
\begin{enumerate}
	\item Input a dataset of \(n\) datapoints \(X = \vec{x}_1, \dots, \vec{x}_n\) with corresponding targets \(y_1, \dots, y_n\).
	\item Repeat instructions~\ref{itm:begin-bp-loop}--\ref{itm:end-bp-loop} for each datapoint \(i = 1, \dots, n\).
	\item \label{itm:begin-bp-loop} Use the training datapoint \(\vec{x}_i\) to make a prediction \(\hat{y}_i\).
	\item Calculate the squared-error for this datapoint \(\varepsilon_i\) by comparing the prediction \(\hat{y}_i\) to the target \(y_i\): \(\varepsilon_i = 1/2 (\hat{y}_i - y_i)^2\).
	\item The target is fixed, and the prediction cannot be directly changed, only the weights.
	      Calculate the gradient of the error with respect to each of the weights \(j = 1, \dots, N\): \(\Delta_j = \diffp{\varepsilon_i}/{w_j}\).
	      The error at each layer only depends on the weights of the later layers, so by applying the chain rule, the calculation can \qt{step back} though the layers until the first weights are reached.
	\item The vector \(\vec{\Delta} = (\Delta_1, \dots, \Delta_N)\) represents the direction in \(N\)-dimensional weight-space that will produce the steepest increase in the error \(\varepsilon_i\), hence taking a step in the direction \(-\vec{\Delta}\) will most reduce the weights.
	\item \label{itm:end-bp-loop} Update the weights \(\vec{w} \leftarrow \vec{w} - \eta \vec{\Delta}\), where \(\eta\) is the step-size hyperparameter, which controls how quickly the algorithm converges.
\end{enumerate}
The backpropagation algorithm will adjust the weights until either the error reaches below a certain threshold or the maximum number of epochs is reached.

\reword{This type of method is called} \qt{stochastic gradient descent} (also called \qt{online gradient descent} or \qt{iterative gradient descent}).
Each datapoint moves the weights in a slightly different direction, so the optimisation process zigzags towards the optimum, rather than taking the steepest path.
An alternative is \qt{batch gradient descent}, where the sum of the squared errors for all the training data is calculated:
\[ \vec{\varepsilon} = \frac{1}{2} \sum_{i = 1}^{n}\left(\hat{y}_i - y_i\right)^2. \]
This means each epoch consists of \reword{only one very efficient step}, and so will converge in fewer epochs, however each epoch takes so long that this takes significantly longer to finish training.
In practice, \qt{minibatches} are used, where a random subset of the datapoints are used to train the model.
This allows for a balance of training time and model quality, as larger batches take longer to train and smaller batches are more \reword{susceptible} to noise~\autocite[59]{thoma2017}.

Estimating the parameters of an \ac{ANN} using any type of gradient descent is guaranteed to find a local optimum given enough time, but is not guaranteed to converge to a global optimum.
It is quite rare for the algorithm to get trapped in a local minimum, but a more common problem is getting stuck at saddle points where the gradient is zero~\autocite[438]{lecun2015}.

\section{Deep Learning} \label{sec:deep-learning}

As computing power has increased exponentially following Moore's law, \ac{ML} has become a more viable and effective method of prediction.
In the 21st century, success has been found in increasing the number of layers in a neural network \reword{or other machine learning method} rather than the complexity of each layer so that the raw data undergoes more levels of abstraction.
These \qt{deep neural networks} \qtc[438]{lecun2015}{can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details}.
As \acp{ANN} get deeper, they reach the \qt{vanishing gradient problem}, where the gradient changes to each layer get smaller and smaller as the backpropagation algorithm gets closer to the input layers.
An \ac{ANN} with too many parameters/too wide will just \qt{remember} the training data, i.e.\ overfit the data and not find any meaningful patterns in the data.

\section{Modern techniques/tools}

\ac{DL} has even made its way into consumer products.
Many modern phones have facial recognition software built in, and \ac{AI} voice assistants are becoming more common in homes.

\ac{DL} is also becoming more accessible, for example with the release of Google's \qt{Tensorflow} package for \ac{DL}~\autocite{abadi2016}, which is now available as \qt{Keras}, a user friendly package for Python~\autocite{chollet2015} and R~\autocite{allaire2018}.

\section{Example}

To demonstrate the possibility of using statistical methods to understand the process of deep learning, we use a simple function \(f(x) = x + 5 \sin(x) + \epsilon\), where \(\epsilon\) is iid Gaussian noise \(\epsilon \distributed \normdist(0, 0.1)\).
256 evenly spread datapoints were drawn from this function, seen in Figure~\ref{fig:sin-x-dataset}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{sin-x-dataset}
	\caption{The true function \(f(x) = x + 5 \sin(x)\) (blue) and the 256 training points (red).}
	\label{fig:sin-x-dataset}
\end{figure}

\subsection{Training a neural network}

This function was learnt by a neural network with 8 layers, each with 10 neurons and the \(\tanh(\cdot)\) activation function, except for the final layer which used a linear activation function.
The result of this learning is seen in Figure~\ref{fig:ann-preds}.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{ann-preds}
	\caption{The true values (blue) and the ANN's predicted values (red).}
	\label{fig:ann-preds}
\end{figure}

\begin{todo}
	Add some more information on how to choose the right size ANN.
\end{todo}

\subsection{Ordering of the datapoints}

Due to the stochastic batch gradient descent used to train the model, if the order of the datapoints is not randomised, then the optimisation algorithm is significantly more likely to get stuck in a local minimum.
An example of this is seen in Figure~\ref{fig:compare-order}, where the same neural network has been trained on the example data which has been ordered in three different ways: increasing \(x\), decreasing \(x\) and randomised.

\begin{figure}[htbp]
	\centering
	\includesvg[width=\figwidth]{compare-order}
	\caption{The output of the same neural network trained on the same data but ordered differently.}
	\label{fig:compare-order}
\end{figure}

\textcite{bengio2009} developed a technique called \qt{curriculum learning} which uses this fact to improve training by starting with easier training examples and slowly working towards more difficult training examples.
In a similar way to how \acp{ANN} were inspired by biological phenomena, this was inspired by how humans are taught simple examples before moving on to harder tasks.

For example, when trained on a text dataset, the algorithm was trained on only texts using the vocabulary of 5,000 most common words.
The vocabulary was expanded by 5,000 more words at intervals, allowing the algorithm to learn new words once it had mastered simpler words.
When compared with an algorithm that had no curriculum, the curriculum-trained model took longer to reach the same error rate as it spent time early on focusing only on simple examples, but achieved a better error rate once the vocabulary was fully expanded.
