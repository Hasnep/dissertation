% !TeX root = ..\dissertation.tex

\chapter{Machine Learning}

We are in an age where there is so much data that in many cases, hand building a model to analyse, find patterns in and draw conclusions from the data is unfeasible, and so we use computers to analyse it in a process called \qt{Machine Learning} (ML)~\autocite[1]{murphy2012}.
Computers are able to do these tasks by combining information in nonlinear ways, but become more powerful when the output of one nonlinear process is fed into another nonlinear process, abstracting the raw data to a higher level.
\qtc[436]{lecun2015}{With the composition of enough such transformations, very complex functions can be learned}.

\section{Neural Networks}

\subsection{The Neuron}

\begin{todo}
	Each neuron performs a weighted sum 
\end{todo}

% In 1957, psychologist Frank Rosenblatt proposed a stochastic electronic brain model, which he called a \qt{perceptron}~\autocite{rosenblatt1957}.
% At the time, most models of the brain were deterministic algorithms which could recreate a single neural phenomenon such as memorisation or object recognition.
% Rosenblatt's biggest criticism of these models was that while a deterministic algorithm could perform a single task perfectly, unlike a biological brain, it could not be generalised to perform many tasks without a lot of substantial changes.
% He described deterministic models of the brain as \qtc[387]{rosenblatt1958}{amount[ing] simply to logical contrivances for performing particular algorithms \cut{} in response to sequences of stimuli}.
% Another way he wanted his synthetic brain to mirror biological brains was the property of redundancy, the ability to have pieces removed and still function, which is not possible for deterministic algorithms where even a small change to a circuit or a line of code can stop all functionality.
% It was a commonly held belief that deterministic algorithms \qtc[387]{rosenblatt1958}{would require only a refinement or modification of existing principles}, but Rosenblatt questioned this idea, believing that the problem needed a more fundamental re-evaluation.
% At the heart of his idea was the \qt{perceptron}, which could -- through repeated training and testing -- receive a set of inputs and reliably give a correct binary response.
% The perceptron was later generalised to the concept of an \qt{(artificial) neuron} (also called a \qt{unit}), which, instead of only giving a binary output, maps a finite number of real inputs to a single real output value.

% Since the invention of \citeauthor{rosenblatt1957}'s perceptron, there have been many alternative models for neurons proposed, including fuzzy neurons, which allow truth values to be on a scale rather than a Boolean value~\autocite{gupta1991}; higher order neurons, which include second or higher products of the inputs~\autocite{rumelhart1986} and spiking neural networks, which use differential equations to emulate the electrical charge of biological neurons~\autocite{maass1997}.
% We will only consider the simple neuron based on Rosenblatt's perceptron.

A neuron is defined by its weight vector~\(\vec{w}\), its bias~\(b\) and its activation function~\(\phi(\x)\).

The stages of a neuron, seen in Figure~\ref{fig:neuron-example} are:
\begin{enumerate}
	\item For an input vector \(\vec{x} \in \reals^n\) and a weight vector \(\vec{w} \in \reals^n\), take a weighted sum of the inputs, called a \qt{linear combiner}.
	      \[ \text{linear combiner} = \sum_{i=1}^{n}{x_i w_i} \]
	\item Then, a bias \(b \in \reals{}\) is added, which translates the output to a suitable range.
	      The bias controls how large the weighted sum needs to be to \qt{activate} the neuron, so this is called the pre-activation \((v)\).
	      \[ \text{pre-activation} = v = \sum_{i=1}^{n}{x_i w_i} + b \]
	\item Finally, an \qt{activation function} (or \qt{limiter function}) \(\phi(\x)\) is applied, which restricts the output to a range and introduces nonlinearity to the system.
	      The activation function must be differentiable if the gradient descent method is used (see Section~\ref{ch:backpropagation}) as gradient descent relies on calculating the gradient of the error.
	      \[ \text{neuron output} = y = \phi\left(\sum_{i=1}^{n}{x_i w_i} + b \right) \]
\end{enumerate}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{A diagram of an example artificial neuron.}
	\label{fig:neuron-example}
\end{figure}

\subsubsection{Activation functions}

Early examples of artificial neurons were built in hardware with the output being a light that was either on or off.
This is equivalent to the \(\operatorname{sign}(\x)\) function, which is now only used for binary classification problems.
It is not useful for connecting to another neuron, as information about the magnitude of the output is lost.
Commonly the sigmoid function \(S(x) = (1 + \exp(-x))^{-1}\), the \(\tanh(\x)\) function or the \qt{softmax} function (a generalisation of the logistic function to vectors) are used.
More recently, a commonly used activation function for deep learning (see Section~\ref{ch:deep-learning}) is the \ac{ReLU} function~\autocite{ramachandran2017}, which is defined as the positive part of its input \(\operatorname{ReLU}(x) = \max(0, x)\).
If the activation function is the identity function, then optimising a neuron is equivalent to performing linear regression.

\subsection{Neural network layers and Backpropagation} \label{ch:backpropagation}

\begin{todo}
	Neurons are connected in layers. \\
	Backpropagation is used to optimise the weights. \\
	Batch backpropagation of all the datapoints takes too long. \\
	Online BP is faster but takes more steps. \\
	Stochastic minibatches can be used to speed up calculations. 
\end{todo}

The weights in a neural network are optimised using the backpropagation algorithm.

\begin{todo}
	Fix my explanation of the backpropagation algorithm.
\end{todo}

\note{Is it possible to add a title to an enumerate?}
For a neural network with \(N\) weights
\begin{enumerate}
	\item Randomly initialise all the weights \(\vec{w} = w_1, \dots, w_N\).
	\item Predict the outputs \(\vec{y}^\text{pred} = y^\text{pred}_1, \dots, y^\text{pred}_n\) using the training data \(X = (\vec{x}_1, \dots, \vec{x}_n)\).
	\item Calculate the error vector \(\vec{\epsilon} = \vec{y}^\text{pred} - \vec{y}^\text{target}\). The error only depends on the output of .
	\item Calculate the gradient of the error \(\vec{\Delta} = \pdv{\vec{\epsilon}}{\vec{w}}\). \(\vec{\Delta}\) represents the direction in \(N\)-dimensional space .
	\item Update the weights \(\vec{w} \leftarrow \).
\end{enumerate}

% To understand the backpropagation algorithm, consider a network with only one output neuron (such as in Figure~\ref{examplenn}), and consider its prediction for a specific datapoint in the training set.
% The squared error for this datapoint is \(\varepsilon = \left(y^\text{target} - y^\text{prediction}\right)^2\).
% The target is fixed, and we cannot directly change the prediction, so to reduce \(\varepsilon\), we can calculate the change we need to make to each weight to reduce the error the most.
% The prediction only depends on the outputs of the neurons on the previous layer and the weights connecting the layers.
% This same logic applies to all of the neurons in the network.
% Assuming that all the activation functions are differentiable, we can apply the chain rule repeatedly to calculate the gradient of the overall error with respect to each weight \(w_j\) in the network.
% This gradient
% \[ \Delta w_j = \pdv{\varepsilon(w_j)}{w_j} \]
% represents the change needed to weight \(w_j\) to most steeply increase the error, so the value \(-\Delta w_j\) is the direction that most steeply decreases the error.
% We scale this amount by a constant hyperparameter \(\eta\), called the \qt{learning rate}, and change each weight accordingly \(w_j = w_j - \eta \Delta w_j\).
% We perform this process for each weight, and then for each datapoint in the training dataset.

% This type of method is called \qt{stochastic gradient descent} (also called \qt{online gradient descent} or \qt{iterative gradient descent}) because it does not calculate the true steepest gradient.
% Each datapoint moves the weights in a slightly different direction, so the optimisation process zigzags towards the optimum, rather than taking the steepest path.
% An alternative is \qt{batch gradient descent}, where the sum of the squared errors for all the training data is calculated and optimised.
% \[ \varepsilon(\vec{w}) = \frac{1}{2} \sum_{i}\left(y_i^\text{target} - y_i^\text{output}\right)^2 \]
% This will converge to a solution in fewer steps, but takes significantly longer to compute.

Estimating the parameters of an \ac{ANN} using either type of gradient descent is guaranteed to find a local optimum given enough time, but is not guaranteed to converge to a global optimum.
It is quite rare for the algorithm to get trapped in a local minimum, but a more common problem is getting stuck at saddle points where the gradient is zero~\autocite[438]{lecun2015}.

\section{Deep Learning}\label{ch:deep-learning}

\begin{todo}
	DL is just ML with more layers
\end{todo}

\note{Rewrite this to more slowly introduce deep learning and be more generally about machine learning than ANNs}
As computing power has increased exponentially following Moore's law, machine learning has become a more viable and effective method of prediction.
In the 21st century, success has been found in increasing the number of layers in a neural network or other machine learning method rather than the complexity of each layer.
These \qt{deep neural networks} \qtc[438]{lecun2015}{can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details}.

\section{Modern techniques/tools}

\ac{DL} has even made its way into consumer products.
Many modern phones have facial recognition software built in, and \ac{AI} voice assistants are becoming more common in homes.

\ac{DL} is also becoming more accessible, with the release of Google's \qt{Tensorflow} package for \ac{DL}~\autocite{abadi2016}, which is now available as \qt{Keras}, a user friendly package for Python~\autocite{chollet2015} and R~\autocite{allaire2018}.

\section{Example}

To demonstrate the possibility of using statistical methods to understand the process of deep learning, we use a simple function \(f(x) = x + 5 \sin(x)\).
256 evenly spread datapoints were taken from this function, and noise following \(\epsilon \distributed \normdist(0, 0.1)\) was applied.

\begin{todo}
	Possibly also use a more complex example?
\end{todo}

\subsection{Training a neural network}

This function was learnt by a neural network with 8 layers, each with 10 neurons and the \(\tanh(\cdot)\) activation function, except for the final layer which used a linear activation function.
The result of this learning is seen in Figure~\ref{fig:ann-output}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{The output of the neural network.}
	\label{fig:ann-output}
\end{figure}

\begin{todo}
	Information on choosing the right size NN
\end{todo}

\subsection{Ordering of the datapoints}

Due to the form of stochastic gradient descent used to train the model, if the order of the datapoints is not randomised, then the optimisation algorithm is significantly more likely to get stuck in a local minimum.
An example of this is seen in Figure~\ref{fig:ann-order}, where the same neural network has been trained on the example data which has been shuffled, reversed, randomised and separated.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{The output of the same neural network trained on the same data but ordered differently.}
	\label{fig:ann-order}
\end{figure}

\textcite{bengio2009} developed a technique called \qt{curriculum learning} which uses this fact to improve training by starting with easier training examples.
