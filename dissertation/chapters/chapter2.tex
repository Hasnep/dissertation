% !TeX root = ..\dissertation.tex

\chapter{Machine Learning}

% We are in an age where there is so much data that in many cases, hand building a model to analyse, find patterns in and draw conclusions from the data is unfeasible, and so we use computers to analyse it in a process called \qt{Machine Learning} (ML)~\autocite[1]{murphy2012}.
% Computers are able to do these tasks by combining information in nonlinear ways, but become more powerful when the output of one nonlinear process is fed into another nonlinear process, abstracting the raw data to a higher level.
% \qtc[436]{lecun2015}{With the composition of enough such transformations, very complex functions can be learned}.

\section{Neural Networks}

\subsection{The Neuron}

\begin{todo}
	Each neuron performs a weighted sum 
\end{todo}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{A diagram of an example artificial neuron.}
	\label{fig:neuron-example}
\end{figure}

\subsection{Neural network layers and Backpropagation}

\begin{todo}
	Neurons are connected in layers, Backpropagation is used to optimise the weights
\end{todo}

\section{Deep Learning}

\begin{todo}
	DL is just ML with more layers 
\end{todo}

\section{Modern techniques/tools}

\begin{todo}
	Section on keras/tensorflow?
\end{todo}

\section{Example}

To demonstrate the possibility of using statistical methods to understand the process of deep learning, we use a simple function \(f(x) = x + 5 \sin(x)\).
256 evenly spread datapoints were taken from this function, and noise following \(\epsilon \distributed \normdist(0, 0.1)\) was applied.

\begin{todo}
Possibly also use a more complex example?
\end{todo}

\subsection{Training a neural network}

This function was learnt by a neural network with 8 layers, each with 10 neurons and the \(\tanh(\cdot)\) activation function, except for the final layer which used a linear activation function.
The result of this learning is seen in Figure~\ref{fig:ann-output}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{The output of the neural network.}
	\label{fig:ann-output}
\end{figure}

\begin{todo}
Information on choosing the right size NN
\end{todo}

\subsection{Ordering of the datapoints}

Due to the form of stochastic gradient descent used to train the model, if the order of the datapoints is not randomised, then the optimisation algorithm is significantly more likely to get stuck in a local minimum.
An example of this is seen in Figure~\ref{fig:ann-order}, where the same neural network has been trained on the example data which has been shuffled, reversed, randomised and separated.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{example-image-golden.pdf}
	\caption{The output of the same neural network trained on the same data but ordered differently.}
	\label{fig:ann-order}
\end{figure}

An example of this being used can be seen in \citeauthor{bengio2009}.
