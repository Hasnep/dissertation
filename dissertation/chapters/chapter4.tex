% !TeX root = ..\dissertation.tex

\chapter{Conclusion}

You can use these methods to:
Approximate the output of the \acl{ANN},
Put the output of the \acl{ANN} in human understandable terms,
Find which terms are significant in the \acl{ANN} model.

If you're trying to discover something about the real world, these techniques all rely on the \acl{ANN} accurately reflecting reality which they probably don't.
The fit of the statistical method will only be as good as the fit of the ANN, these techniques cannot diagnose a bad \acl{ANN}.

% One problem with using a \ac{GP} is that prediction with \(n\) training datapoints involves inverting an \(n \times n\) matrix, which makes it impractical for predicting many points quickly.
% On the other hand, \aclp{ANN} take a long time to train, but once trained, predication only involves calculating the output of each neuron, so predictions can be made very quickly.
% This means that this technique is not useful for live situations where an \aclp{ANN} is being trained and an analysis is needed alongside the training.
% However, after an \acl{ANN} is trained, a \ac{GP} can be fitted and predictions pre-calculated which can then be used live.

\section{How well have each of the attempts worked?}

The stepwise regression was not as good as the \ac{LASSO} technique, which worked better.
\acp{GP} were as good as expected.

It is not clear how well these techniques would transfer to higher dimensions and more complex situations as the \ac{GP} took a long time to fit, and the regression didn't fit that well on a simple example.

\section{What could be improved?}

\section{How useful would more research on this topic be?}

\section{Future research}

These techniques rely on the fit of the \acl{ANN} to recover the original information, and the \acl{ANN} could definitely fit better.

If using this, it would be sensible to first perform sensitivity analysis to find the significant variables.

For more complex applications, the use of \acp{GP} could be extended to deep (also known as hierarchical) \acp{GP}, which are analogous to \acp{DNN} (see Section~\ref{sec:deep-learning}).
\reword{They involve} chaining the output of one \ac{GP} into another to better model nonlinearity~\autocite{damianou2013}.

One simple possibility to extend the models proposed would be to use multiple regression to find a trend and then use this as a mean function in a \ac{GP}.
This should allow the regression to capture most of the pattern and then the \ac{GP} will provide a flexible way to capture the residuals.

\section{Conclusion's conclusion}

Deep learning is fast becoming a \reword{ubiquitous tool} in many aspects of modern life --- sometimes clearly visible, as with driverless cars, but in some cases more discreetly, such as the use of \ac{ML} algorithms in American courts.
As \acp{DNN} were designed to mimic a biological brain, a \reword{phenomenon} that is still not well understood, they are a \qt{black box} \reword{whose} reasoning is impossible to understand.
Statistical methods such as \acp{GP} may offer a way to look inside this black box, as they offer a similar flexibility and wide range of uses, and are much more easily interpreted by humans.
So far, much of the work that has been done involving \acp{GP} and \ac{ML} has been comparative, rather than using one to model the other.
