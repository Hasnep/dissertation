% !TeX root = ../dissertation.tex

\chapter{Conclusion}

The methods discussed in this project have been successfully fitted to the output of a deep neural network and used to learn more about its properties.
The form of the equation used to train the neural network was recovered by fitting a linear model with a series of basis functions.
Regularisation techniques allowed the model to be simplified further to only a few interpretable terms.
The \ac{LASSO} worked much better than stepwise regression to select a few important terms and also allows for more fine control of the regularisation strength.
\acp{GP} also were used to provide a nonparametric method to understand the neural network, and also managed to capture the residuals of the \ac{LASSO} model.

These tests have relied on the fit of the neural network to be sufficiently good that the original function can be recovered.
In realistic applications, the process that produced the data is unknown, so there is no way to check how accurate the regression's understanding is.
Further testing with more complex training datasets would need to be done to see how well these techniques generalise to higher dimensions and more complex functions.
% If used on real datasets, it would be sensible to first perform sensitivity analysis to find significant variables.

For more complex applications, the use of \acp{GP} could be extended to deep (also known as hierarchical) \acp{GP}, which are analogous to deep neural networks.
They involve chaining the output of one \ac{GP} into another to better model nonlinearity~\autocite{damianou2013}.

Deep learning is fast becoming a widespread tool in many aspects of modern life --- sometimes clearly visible, as with driverless cars, but in some cases more discreetly, such as the use of machine learning algorithms in the U.S.\ judicial system.
As deep neural networks were designed to mimic biological brains which are still not well understood, they are a \qt{black box} whose reasoning is impossible to understand.
Statistical methods such as \acp{GP} may offer a way to look inside this black box, as they offer a similar flexibility and wide range of uses, and are much more easily interpreted by humans.
So far, much of the work that has been done involving \acp{GP} and machine learning has been comparative, rather than using one to model the other.
