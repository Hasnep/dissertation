% !TeX root = ..\dissertation.tex

\chapter{Introduction}

Since the turn of the millennium, machine learning, and particularly \acl{DL} have been able to solve many problems that were once thought impossible for a computer and surpass traditional algorithms in many contexts.
Machines can now identify objects~\autocite{li2018}, beat humans at PSPACE-hard games such as Go~\autocite{chao2018} and even drive cars~\autocite{gerla2014}.
These tasks are so complex that human designed algorithms quickly become infeasible, so instead, we design a process that uses datasets of correct examples to teach the machine how to respond to patterns~\autocite[1]{murphy2012}.
One of these machine learning algorithms is the artificial neural network, which takes inspiration from the neural structure of the biological brain.
Neural networks consist of units called \qt{neurons}, which individually perform a simple nonlinear operation, but when interconnected can understand more complex structures~\autocite[436]{lecun2015}.
Due to the abstracted nature of a machine learning algorithm's calculations, it can be difficult to provide a human understandable explanation for its reasoning, and often it is impossible.

As these algorithms are placed in more real world scenarios and in control of important infrastructure and even human lives, the need to understand what is happening inside the \qt{black box} becomes more important.
Autonomous vehicles are becoming more common, and will face corner cases which no human could have predicted and preprogrammed a response.
In cases where a driverless car does something unforeseen, it will be useful to have some understanding of what the algorithm was \qt{thinking} when it made the decision.
machine learning systems which aid in probation and parole decisions in the U.S.\ are now being trialled for sentencing, but have come under a lot of criticism for being racially biased~\autocite{christin2015}.
The question of how an algorithm can be \qt{held accountable} has been raised~\autocite[9]{christin2015}, but it is not known what accountability looks like for an artificial mind.
